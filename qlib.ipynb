{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![qlib](figure/qlib_flow.png)\n",
    "![junk](figure/junk.png)\n",
    "# Categories\n",
    "- Gradient Boosting Decision Trees (GBDT):\n",
    "\n",
    "    - XGBoost (Tianqi Chen, et al. KDD 2016)\n",
    "    - LightGBM (Guolin Ke, et al. NIPS 2017)\n",
    "    - CatBoost (Liudmila Prokhorenkova, et al. NIPS 2018)\n",
    "    - DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)\n",
    "- Neural Networks (NN):\n",
    "\n",
    "    - Feedforward Neural Networks (MLP) based on PyTorch\n",
    "    - Recurrent Neural Networks (RNN):\n",
    "        - Long Short-Term Memory (LSTM) based on PyTorch (Sepp Hochreiter, et al. Neural computation 1997)\n",
    "        - Gated Recurrent Unit (GRU) based on PyTorch (Kyunghyun Cho, et al. 2014)\n",
    "        - Attention-based LSTM (ALSTM) based on PyTorch (Yao Qin, et al. IJCAI 2017)\n",
    "        - Kernelized Recurrent Neural Networks (KRNN) based on PyTorch\n",
    "        - Adaptive Recurrent Neural Networks (ADARNN) based on PyTorch (YunTao Du, et al. 2021)\n",
    "    - Graph Neural Networks (GNN):\n",
    "        - Graph Attention Networks (GATs) based on PyTorch (Petar Velickovic, et al. 2017)\n",
    "    - Convolutional Neural Networks (CNN):\n",
    "        - Temporal Convolutional Networks (TCN) based on PyTorch (Shaojie Bai, et al. 2018)\n",
    "    - Transformer-based Models:\n",
    "        - Transformer based on PyTorch (Ashish Vaswani, et al. NeurIPS 2017)\n",
    "        - Localformer based on PyTorch (Juyong Jiang, et al.)\n",
    "    - Spatial-Temporal Models:\n",
    "        - Hierarchical Spatial-Temporal (HIST) based on PyTorch (Wentao Xu, et al. 2021)\n",
    "    - Attentional Models:\n",
    "        - Attentional Deep Learning (ADD) based on PyTorch (Hongshun Tang, et al. 2020)\n",
    "        - Interpretable Graph Multi-Task Framework (IGMTF) based on PyTorch (Wentao Xu, et al. 2021)\n",
    "    - Other NN architectures:\n",
    "        - Sandwich based on PyTorch\n",
    "- Other Models:\n",
    "    - Temporal Fusion Transformers (TFT) based on TensorFlow (Bryan Lim, et al. International Journal of Forecasting 2019)\n",
    "    - TabNet based on PyTorch (Sercan O. Arik, et al. AAAI 2019)\n",
    "    - Temporal Relational Attention (TRA) based on PyTorch (Hengxu Dong, et al. KDD 2021)\n",
    "    - Structured Feature Manipulation (SFM) based on PyTorch (Liheng Zhang, et al. KDD 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strength & Weakness\n",
    "\n",
    "## Gradient Boosting Decision Trees (GBDT):\n",
    "\n",
    "### XGBoost:\n",
    "\n",
    "- Pros:\n",
    "    - Well-optimized, efficient implementation with parallelization and regularization techniques.\n",
    "    - Often achieves state-of-the-art results in structured/tabular data competitions.\n",
    "    - Supports various objective functions and custom evaluation metrics.\n",
    "    - Offers advanced features like monotonicity constraints and handling missing values.\n",
    "\n",
    "- Cons:\n",
    "    - Can be computationally expensive for large datasets due to its inherently sequential nature.\n",
    "    - Prone to overfitting if not carefully tuned, especially with noisy or imbalanced data.\n",
    "    - Limited support for handling categorical features directly without preprocessing.\n",
    "    - Interpretability diminishes with increasing model complexity.\n",
    "\n",
    "### LightGBM:\n",
    "\n",
    "- Pros:\n",
    "    - Optimized for speed and efficiency with distributed computing and GPU support.\n",
    "    - Handles categorical features natively with efficient tree building algorithms.\n",
    "    - Less memory usage compared to other GBDT implementations, making it suitable for large datasets.\n",
    "    - Offers built-in feature importance calculation.\n",
    "\n",
    "- Cons:\n",
    "    - Sensitive to hyperparameters, particularly the number of leaves and maximum depth.\n",
    "    - May require more data preprocessing compared to XGBoost for optimal performance.\n",
    "    - Limited support for custom loss functions and evaluation metrics compared to XGBoost.\n",
    "    - Less mature compared to XGBoost in terms of community support and documentation.\n",
    "\n",
    "### CatBoost:\n",
    "\n",
    "- Pros:\n",
    "    - Handles categorical features automatically without manual preprocessing.\n",
    "    - Robust to overfitting due to its novel implementation of ordered boosting and symmetric trees.\n",
    "    - Supports GPU training for faster computation.\n",
    "    - Provides advanced features like robust model selection and parameter tuning.\n",
    "\n",
    "- Cons:\n",
    "    - Slower training speed compared to other GBDT libraries, especially with large datasets.\n",
    "    - Limited support for custom loss functions and evaluation metrics compared to XGBoost.\n",
    "    - Higher memory usage compared to LightGBM due to its categorical feature handling.\n",
    "    - May not consistently outperform XGBoost and LightGBM in all scenarios.\n",
    "\n",
    "## Neural Networks (NN):\n",
    "\n",
    "### Feedforward Neural Networks (MLP) based on PyTorch:\n",
    "\n",
    "- Pros:\n",
    "    - Highly flexible architecture suitable for various tasks including regression, classification, and function approximation.\n",
    "    - Easy to implement and experiment with different network structures and hyperparameters.\n",
    "    - Effective for modeling non-linear relationships and complex patterns in data.\n",
    "    - Suitable for both small and large datasets.\n",
    "\n",
    "- Cons:\n",
    "    - Prone to overfitting, especially with deep architectures and limited data.\n",
    "    - Requires careful tuning of hyperparameters like learning rate, batch size, and regularization.\n",
    "    - Interpretability is limited compared to simpler models like linear regression or decision trees.\n",
    "    - Training can be computationally intensive, particularly for deep networks.\n",
    "\n",
    "### Recurrent Neural Networks (RNN):\n",
    "\n",
    "- Pros:\n",
    "    - Suitable for sequential data processing tasks such as time series prediction, natural language processing, and speech recognition.\n",
    "    - Can capture temporal dependencies and long-range dependencies in data.\n",
    "    - Offers various architectures like LSTM and GRU, each with its advantages for different tasks.\n",
    "    - Supports variable-length inputs and outputs.\n",
    "\n",
    "- Cons:\n",
    "    - Prone to vanishing and exploding gradients, which can hinder training stability and learning long-term dependencies.\n",
    "    - Computationally intensive, especially with deep architectures and long sequences.\n",
    "    - Sensitive to the choice of activation functions and initialization methods.\n",
    "    - May suffer from issues like gradient decay and information loss over long sequences.\n",
    "\n",
    "### Graph Neural Networks (GNN):\n",
    "\n",
    "- Pros:\n",
    "    - Effective for learning representations of graph-structured data such as social networks, molecular graphs, and citation networks.\n",
    "    - Can capture both node-level and graph-level information through message passing mechanisms.\n",
    "    - Offers flexibility in designing architectures for different graph-based tasks.\n",
    "    - State-of-the-art performance in tasks like node classification, link prediction, and graph classification.\n",
    "\n",
    "- Cons:\n",
    "    - Limited scalability for large graphs due to computational complexity.\n",
    "    - Vulnerable to over-smoothing and information loss in deep architectures.\n",
    "    - Requires careful design and tuning of aggregation functions and neighborhood sampling strategies.\n",
    "    - Interpretability can be challenging, especially with deep GNN architectures.\n",
    "\n",
    "### Convolutional Neural Networks (CNN):\n",
    "\n",
    "- Pros:\n",
    "    - Effective for tasks involving grid-like structured data such as images, video frames, and 1D signals.\n",
    "    - Can automatically learn hierarchical features through convolutional and pooling layers.\n",
    "    - Translation-invariant properties make them suitable for tasks like object recognition and image classification.\n",
    "    - Supports transfer learning and fine-tuning pretrained models on new datasets.\n",
    "\n",
    "- Cons:\n",
    "    - Limited in handling sequential and graph-structured data compared to RNNs and GNNs.\n",
    "    - Requires large amounts of labeled data for training, especially for deep architectures.\n",
    "    - Vulnerable to issues like overfitting, especially with small datasets and complex models.\n",
    "    - Interpretability can be challenging, particularly with deep convolutional architectures.\n",
    "\n",
    "### Transformer-based Models:\n",
    "\n",
    "- Pros:\n",
    "    - Highly parallelizable architecture suitable for processing long sequences with self-attention mechanisms.\n",
    "    - State-of-the-art performance in various natural language processing tasks including machine translation, text generation, and sentiment analysis.\n",
    "    - Supports efficient training and inference through techniques like attention masking and scaled dot-product attention.\n",
    "    - Offers flexibility in model size and depth, allowing trade-offs between performance and computational resources.\n",
    "\n",
    "- Cons:\n",
    "    - Computationally expensive, especially with large vocabularies and long sequences.\n",
    "    - Limited ability to handle structured data and non-sequence data compared to CNNs and GNNs.\n",
    "    - Interpretability can be challenging due to the complexity of attention mechanisms and multiple layers.\n",
    "    - Requires large-scale pretraining data for achieving state-of-the-art performance, limiting applicability in low-resource scenarios.\n",
    "\n",
    "### Spatial-Temporal Models:\n",
    "\n",
    "- Pros:\n",
    "    - Effective for capturing both spatial and temporal dependencies in data such as video sequences, sensor data, and spatiotemporal graphs.\n",
    "    - Offers flexibility in designing architectures for different tasks including video action recognition, trajectory prediction, and dynamic graph modeling.\n",
    "    - Can capture both short-term and long-term dependencies through convolutional and recurrent layers.\n",
    "    - State-of-the-art performance in tasks like video understanding, human pose estimation, and traffic forecasting.\n",
    "\n",
    "- Cons:\n",
    "    - Computationally intensive, especially with large-scale datasets and complex architectures.\n",
    "    - Requires careful design and tuning of hyperparameters like kernel sizes, dilation rates, and temporal receptive fields.\n",
    "    - Vulnerable to issues like overfitting, especially with limited training data and deep architectures.\n",
    "    - Interpretability can be challenging due to the complexity of spatial-temporal relationships and model architectures.\n",
    "\n",
    "### Attentional Models:\n",
    "\n",
    "- Pros:\n",
    "    - Effective for modeling complex relationships and dependencies in data through attention mechanisms.\n",
    "    - Can selectively attend to relevant parts of input data, improving performance in tasks like machine translation, question answering,  and image captioning.\n",
    "    - Offers flexibility in incorporating attention mechanisms into various neural network architectures including RNNs, CNNs, and  Transformers.\n",
    "    - State-of-the-art performance in tasks requiring long-range dependencies and context understanding.\n",
    "\n",
    "- Cons:\n",
    "    - Computationally expensive, especially with large-scale datasets and deep architectures.\n",
    "    - Vulnerable to issues like overfitting, especially with limited training data and complex attention mechanisms.\n",
    "    - Requires careful design and tuning of attention mechanisms, including attention heads, layers, and attention scores.\n",
    "    - Interpretability can be challenging due to the complexity of attention distributions and aggregation methods.\n",
    "\n",
    "## Other Models (TFT, TabNet, SFM, etc.):\n",
    "\n",
    "- Pros:\n",
    "    - Specialized architectures tailored for specific tasks like time series forecasting, tabular data processing, and structured feature manipulation.\n",
    "    - Can offer state-of-the-art performance in their respective domains with appropriate data preprocessing and model tuning.\n",
    "    - Offers flexibility in incorporating domain-specific knowledge and constraints into model architectures.\n",
    "    - Can provide interpretable insights into model predictions and feature importance through specialized techniques and visualizations.\n",
    "\n",
    "- Cons:\n",
    "    - Limited applicability outside their specialized domains, potentially requiring additional effort for adaptation to new tasks and datasets.\n",
    "    - May require specific data preprocessing or feature engineering steps for optimal performance.\n",
    "    - Performance heavily depends on hyperparameter tuning and architecture choices, requiring domain expertise and experimentation.\n",
    "    - Less established compared to mainstream architectures like GBDT, NNs, and Transformers, potentially leading to fewer resources and community support.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
